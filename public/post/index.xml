<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Anything is |Data| is Anything</title>
    <link>/post/</link>
    <description>Recent content in Posts on Anything is |Data| is Anything</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Jun 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Part II: Chinese Classics&#39; Word/Network Plots</title>
      <link>/blog/2018-06-08-part-ii-chinese-classics-word/network-plots/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-06-08-part-ii-chinese-classics-word/network-plots/</guid>
      <description>This is a continuation in my series of exploratory text analysis of 3 Chinese classic works. In the previous post, I calculated word counts for each book, and visualized common words using bar charts. This time, I’d like to examine word use across the texts with network visualization. The goal is to help see what’s common and what’s different between the texts regarding word usage.
Network visualization is particularly helpful for discovering simularities and differences between objects - this is because nodes and edges can form connections and clusters (or stay isolated).</description>
    </item>
    
    <item>
      <title>Plotting Word Bigrams with 3 Chinese Classics</title>
      <link>/blog/2018-06-01-plotting-word-bigrams-with-3-chinese-classics/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-06-01-plotting-word-bigrams-with-3-chinese-classics/</guid>
      <description>In the last post, we saw frequencies of the most common words in the Analects, Zhuangzi, and Mozi texts. The faceted plot did an excellent job of capturing a generic “theme” of each text. However, I wondered how the results might change when plotting bigrams (2 word combinations of adjacent words) as opposed to single values.
This is where I ran into a problem with Tidytext – although it worked fine for tokenizing Chinese text into single character tokens, it did not perform as well at separating the text into bigrams.</description>
    </item>
    
    <item>
      <title>A Tidytext Analysis of 3 Chinese Classics</title>
      <link>/blog/2018-05-29-a-tidytext-analysis-of-3-chinese-classics/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-29-a-tidytext-analysis-of-3-chinese-classics/</guid>
      <description>For a long time I’ve admired the tidytext package and its wonderful companion book Text Mining with R. After reading it I thought, “Why not undertake a project of Chinese text analysis?” I am deeply interested in Chinese philosophy but I decided to keep the analysis narrow by selecting just three works - The Analects, Zhuangzi, and the Mozi.
Following similar pace with Tidytext, I first download my data. Here I use my package ctextclassics and specifically, the function get_books(c(.</description>
    </item>
    
    <item>
      <title>Ctextclassics, my First Package</title>
      <link>/blog/2018-05-17-ctextclassics-my-first-package/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-17-ctextclassics-my-first-package/</guid>
      <description>My latest update is a milestone! I have authored my first ever R package which is an API caller for ctext.org. Ctext hosts numerous pre-modern Chinese texts and my package makes them available to you. The scope is broad, but think philosophical works in Confucianism, Daoism, Legalism, military doctrines, history compilations, works in medicine, and many more.
The three main functions of ctextclassics are get_chapter(&amp;quot;book&amp;quot;, &amp;quot;chapter&amp;quot;) ,get_chapters(&amp;quot;book&amp;quot;, chapters), get_books(&amp;quot;book&amp;quot;) and the internal dataframe book_list which shows the available texts.</description>
    </item>
    
    <item>
      <title>Scraping for a Booklist of the Chinese Classics</title>
      <link>/blog/2018-05-08-scraping-for-a-booklist-of-the-chinese-classics/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-08-scraping-for-a-booklist-of-the-chinese-classics/</guid>
      <description>Last week I was considering a project that would be interesting and unique. I decided I would like to do a text analysis on classical Chinese texts, but wasn’t sure what kind of analysis regarding which texts. I decided to keep it small - and use five of the “core” Chinese classics - The Analects, The Mengzi, Dao De Jing, Zhuangzi, and Mozi. While there are many books in Confucianism, Daoism, and Moism, these texts are often used as the most representative examples of each “genre”.</description>
    </item>
    
    <item>
      <title>On Relocating to Github/Netlify</title>
      <link>/blog/2018-05-06-on-relocating-to-github/netlify/</link>
      <pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-06-on-relocating-to-github/netlify/</guid>
      <description>Deep, labored breathing
Hello everyone, this is the opening post on my new blog, which I’m relocating from Wordpress to GitHub Pages and Netlify. It’s so nice I’ve given it a name - because nice things have names!
But, why was I panting? The relocation effort wasn’t easy. Why did I follow through with it? Becuase it is worth the effort. This post is evidence of my victory. Now please, let me explain.</description>
    </item>
    
    <item>
      <title>Visualizing European WW1 Defense Treaties with iGraph</title>
      <link>/blog/2018-04-01-visualizing-european-ww1-defense-treaties-with-igraph/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-01-visualizing-european-ww1-defense-treaties-with-igraph/</guid>
      <description>I suddenly got bit by a bug to learn about network analysis. So I recalled the Correlates of War Project having a dataset about alliances. I decided to revisit them and download the data for this new project, which you can do too.
To start small, I decided to visualize a certain topic, e.g., European Defense Treaties relating to World War I. For that purpose I filtered the dataset for treaties that occured between 1878 and 1914.</description>
    </item>
    
    <item>
      <title>Plotting Fortune 500 HQ&#39;s in R</title>
      <link>/blog/2018-01-29-plotting-fortune-500-hqs-in-r/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-29-plotting-fortune-500-hqs-in-r/</guid>
      <description>Today I’d like to work a little on geospatial mapping in R, so I’ve chosen a small dataset (only 256 kb) that can be plotted on a map. It the location information of Fortune 500 company headquarters in the US. You can download it from here.
R has several choices for plotting geospatial information. Here I use ggmap, however in the future I’ll check out the raster and sp packages. Anyway, let’s get started by loading in and cleaning the data.</description>
    </item>
    
    <item>
      <title>Top MBA Programs by US News</title>
      <link>/blog/2017-07-01-top-mba-programs-by-us-news/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-07-01-top-mba-programs-by-us-news/</guid>
      <description>Somebody once asked me for reccomendations on MBA programs based on rank and tuition. I didn’t have any information on hand, but knew how toget it. Webscraping.
Webscraping is an immensly useful tool for gathering data from webpages, when it isn’t hosted on an API or stored in a file somewhere. R’s best tool for webscraping is Rvest.
So I decided to scrape information on the US News website for university rankings, which has at least 20 pages of MBA probrams available.</description>
    </item>
    
    <item>
      <title>International Organization Growth Trend</title>
      <link>/blog/2017-01-24-international-organization-growth-trend/</link>
      <pubDate>Tue, 24 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-01-24-international-organization-growth-trend/</guid>
      <description>Note: This was my first “project” in R - and I had simple goals: learning data wrangling of a not-so-tidy dataset, and performing basic visualization. I wanted to learn the tidyverse set of packages, use pipe commands, and convert data from wide to long formats for different purposes.
I chose to learn the tidyverse because it gets all sorts of wrangling done about 90% of the time, (or so its said)</description>
    </item>
    
  </channel>
</rss>