<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Anything is |Data| is Anything</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Anything is |Data| is Anything</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predicting Churn Using Tree Models</title>
      <link>/blog/2019-01-01-predicting-churn-using-tree-models/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-01-predicting-churn-using-tree-models/</guid>
      <description>&lt;p&gt;Today I want to predict churn using data from a hypothetical telecom company. Although it isn’t real life data, it is based on real life data. The data are spread across 19 columns — 14 continuous, 4 categorical, and the outcome variable for prediction - “churn”. The dataset is small, with 3333 rows for training and 1667 for testing.&lt;/p&gt;
&lt;p&gt;Before modeling, I need to explore the data structure – the distributions, class balances/imbalances, relationships between variables, etc. Are there any visual patterns in the data? How does churn relate to the continuous and discrete variables? These are questions I am asking at this initial stage. Let’s start with a series of histograms.&lt;/p&gt;
&lt;div id=&#34;data-overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Overview&lt;/h2&gt;
&lt;center&gt;
&lt;img src = &#34;/1_distributions.png&#34; width = 550 height = 550&gt;
&lt;/center&gt;
&lt;p&gt;Most continuous predictors appear normally distributed. However, there are some exceptions “Customer service calls” and “total international calls” are somewhat right skewed, and the majority of values for “numer of mail messages” are all “0”, meaning it is probably a zero variance predictor. For the most part, these data are well behaved.&lt;/p&gt;
&lt;p&gt;Let’s examine how these same predictors relate to churn. A density plot showing the variables’ distributions by class will suffice.&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/2_distributions.png&#34; width = 600 height = 600&gt;
&lt;/center&gt;
&lt;p&gt;This density plot shows that “positive” churn has symmetrical distributions with that of the non-churners for most numeric predictors. (Note this is a density plot, so it depicts proportions, not quantities, of each class across a range of values.) We only see separation between red and green in amount of minutes per day and day charges, but these two variables clearly repeat the same information. One should probably be removed. Let’s continue by looking at how churn relates to the categorical variables.&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/3_categorical_distributions.png&#34; width = 600 height = 500&gt;
&lt;/center&gt;
&lt;p&gt;Categorical predictors are informative – &lt;strong&gt;Look at how many customers on an international plan end up churning!&lt;/strong&gt; On the flipside, notice how churn outcome is unrelated to area code, but somewhat related to if they have voice mail. International plan and voice mail look like they will be useful predictors. Is “state” information useful at all?&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/4_state_distributions.png&#34;&gt;
&lt;/center&gt;
&lt;p&gt;State information sure doesn’t look useful. From just looking at totals, it’s hard to see any kind of pattern. Also there doesn’t seem to be any interesting variation, even if we randomly scrambled them all. Another way to see if there is a here is to plot the churn data on a choropleth (a shaded map) plot.&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/5_states.png&#34;&gt;
&lt;/center&gt;
&lt;p&gt;I still cannot determine any visual pattern for churn amongst states. This variable probably won’t be useful for most models. We can also check if there are obvious patterns between the continuous variables. Usually a panel of scatterplots can be made using &lt;code&gt;pairs()&#39; or &#39;GGally::ggpairs()&lt;/code&gt;. However, these functions dont work out of the box due to having too many variables.&lt;/p&gt;
&lt;p&gt;With a bit of coding I aggregate the scatterplots into panels of 30, and produce a total of 4 grids like the two below. No need to clog up the page with all of them, most show a little clustering in some variables regarding the yes/no churn outcomes, and 1 or 2 instances of collinearity.&lt;/p&gt;
&lt;center&gt;
&lt;div class = &#34;&#34;; display: inline-block&gt; &lt;img src = &#34;/pairs1.png&#34; width = 300, height = 300&gt;
&lt;img src = &#34;/pairs2.png&#34; width = 300 height = 300&gt;&lt;/div&gt;
&lt;/center&gt;
&lt;p&gt;Clearly, Churn &lt;strong&gt;doesn’t appear to be linearly separable&lt;/strong&gt;, meaning a linear classifier might not suffice. However, there does appear to be some clusering for some of these scatterplots. Still, tree models and KNN might perform better. My last check is to look at class balance for the outcome variable.&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/churnclassplot.jpeg&#34; width = 400 height = 250&gt;
&lt;/center&gt;
&lt;p&gt;&lt;strong&gt;Churn classes are very imbalanced.&lt;/strong&gt; Normally this can be fixed through up/down sampling the data, but here the training set has already been provided. Resampling here would bias predictions, so I’ll just bite the bullet.&lt;/p&gt;
&lt;p&gt;Before modelling, let’s recap:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most continuous variables are evenly distributed.&lt;/li&gt;
&lt;li&gt;Continuous predictors do not appear linearly seprable on the face of things, but there is some clustering.&lt;/li&gt;
&lt;li&gt;There’s class imbalance in the outcome variable &lt;code&gt;churn&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;International plan and voice mail seem informative, as does total day charges. Perhaps a few states with super high or low vlues will be informarive in a model that uses an expanded predictor set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Now lets try to build some models to help predict churn&lt;/strong&gt; I’ll use two versions of predictor sets to fit models, one with the regular set of variables, and another with an “expanded set” where categorical variables are coded into binary (1-0) values.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modelling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modelling&lt;/h2&gt;
&lt;p&gt;Since data didn’t show linearity, I’ll start with a K Nearest Neighbors. Data needs to be numeric (so for classification, I’ll use binary values for all categorical variables) and data need to be preprocessed with scaling so that variables with higher values don’t outweigh the variables with lower values. The only tuning paramater is K, which will be automatically tuned across 20 values of tunelength.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Expanded set
knn_expanded &amp;lt;- train(training_expanded, training_outcome, method = &amp;quot;knn&amp;quot;, preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;),
                   trControl = ctrl, tuneLength = 20, metric = &amp;quot;Sens&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optimal model has a paramater of K=5. ROC is .82, sensitivity .28, and specificty .98. Although ROC (area under the curve) is high, sensitivity is way too low. It seems we cannot effectively use KNN to identify customers that will churn.&lt;/p&gt;
&lt;p&gt;Next let’s try a tree/rules model using C50. The model can fit both decision trees and rules based trees. It also has the option of “winnow” which means it prunes away predictors it deems less useful.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c50_regular &amp;lt;- train(training_regular, training_outcome, method = &amp;quot;C5.0&amp;quot;, trControl = ctrl, tunegrid = expand.grid(trials = seq(25, 50, by = 2), model = c(&amp;quot;tree&amp;quot;, &amp;quot;rules&amp;quot;), winnow = c(&amp;quot;TRUE&amp;quot;, &amp;quot;FALSE&amp;quot;)), metric = &amp;quot;Sens&amp;quot;)

c50_expanded &amp;lt;- train(training_expanded, training_outcome, method = &amp;quot;C5.0&amp;quot;, trControl = ctrl, tuneGrid = expand.grid(trials = seq(25, 50, by = 2), model = c(&amp;quot;tree&amp;quot;, &amp;quot;rules&amp;quot;), winnow = c(&amp;quot;TRUE&amp;quot;, &amp;quot;FALSE&amp;quot;)), metric = &amp;quot;Sens&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Churn classes in this dataset are heavily imbalanced and that simply by predicting “No”, a model can achieve a high ROC like KNN did. What we want to maximize is &lt;strong&gt;sensitivity&lt;/strong&gt;, and for that, the expanded C50 does a great job compared to the regular set. It fits a rules model with no winnow, and achieves a sensitivity value of .75, specificity of .98, and ROC of .909.&lt;/p&gt;
&lt;p&gt;Next up, a random forest. Again, I’ll fit two models, using the normal set and the expanded set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_regular &amp;lt;- train(training_regular, training_outcome, method = &amp;quot;rf&amp;quot;, ntree = 1500, trControl = ctrl, tuneLength = 20, metric = &amp;quot;Sens&amp;quot;, verbose = FALSE)


rf_expanded &amp;lt;- train(training_expanded, training_outcome, method = &amp;quot;rf&amp;quot;, ntree = 1500, trControl = ctrl, tuneGrid = expand.grid(mtry = seq(25, 60, by = 2)), metric = &amp;quot;Sens&amp;quot;, verbose = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optimal random forest model is the model using the expanded set, and it uses a paramater of mtry = 17. ROC is .89, sensitivity is .82, and specificity is .90. The quality of fit is good, but the model for the regular variable set performed worse.&lt;/p&gt;
&lt;p&gt;Now let’s try a gradient boosted tree model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gbm_regular &amp;lt;- train(training_regular, training_outcome, method = &amp;quot;gbm&amp;quot;, trControl = ctrl, tuneLength = 20, metric = &amp;quot;Sens&amp;quot;, tuneLength = 20 verbose = FALSE)

gbm_expanded &amp;lt;- train(training_expanded, training_outcome, method = &amp;quot;gbm&amp;quot;, trControl = ctrl, tuneLength = 20, metric = &amp;quot;Sens&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Performance for the model trained on the expanded predictor set is slightly higher than the c50 model in terms of sensitivity. Let’s compare the sensitivity values of each model in a boxplot. Since the models were cross validated over 10 folds, we can be sure that these sensitivity estimates are stable.&lt;/p&gt;
&lt;p&gt;Models built using the expanded predictor sets (categorical variables expanded into columns using hot coded binary values) performed much better than the models built on regular sets. Expanded random forest and c50 to be the best performers on the testing sets. Let’s see how they perform on hold out testing data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Use models to predict test data, evaluate performance.
knn_pred &amp;lt;- predict(knn_expanded, newdata = testing_expanded)
c50_normal_pred &amp;lt;- predict(c50_regular, newdata = testing_regular)
c50_expanded_pred &amp;lt;- predict(c50_expanded, newdata = testing_expanded)
rf_normal_pred &amp;lt;- predict(rf_regular, newdata = testing_regular)
## Make list of model predictions
models &amp;lt;- list(&amp;quot;knn&amp;quot; = knn_pred, &amp;quot;c50&amp;quot; = c50_normal_pred, &amp;quot;c50 exp.&amp;quot; = c50_expanded_pred, &amp;quot;random forest&amp;quot; = rf_normal_pred)

##Make list of all model predictions
predictions_list &amp;lt;- list(&amp;quot;knn&amp;quot; = knn_pred, &amp;quot;c50_normal&amp;quot; = c50_normal_pred, &amp;quot;c50_expanded&amp;quot; = c50_expanded_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;center&gt;
&lt;img src = &#34;/sensitivity.png&#34; width = 600 height = 600&gt;
&lt;/center&gt;
&lt;p&gt;The c50 model built on the expanded dataset achieves the highest sensitivity, at .741.&lt;/p&gt;
&lt;p&gt;There are a number of ways the sensitivity of these models could be improved. Sometimes resampling the data to balance the outcome variable can help. Also, setting a new probability threshold on the predictions would help if the class balance means there is a low signal for the class you’re trying to predict.&lt;/p&gt;
&lt;p&gt;Say, instead of only predicting “Yes” if it has a probability of .5 or greater, we can lower the probability threshold of “evidence needed” to predict yes, to say .3, or .4, etc. Doing so will not change the model, but it will come at the cost of specificity. A rudimentary representation can be seen below, using the C50 model with the expanded predictor set.&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/roc_curve.png&#34; width = 600 height = 600&gt;
&lt;/center&gt;
&lt;p&gt;Above are probability thresholds set at .5, .2, and .1. Notice how sensitivity significantly improves if we determine a “yes” cutoff lower than .5, and that specificity isn’t impacted by this. However, when setting the threshold too low (.1), sensitivity tapers off, and comes with a much greater loss to specificity. At that point, half of our predictions would be false positives. Therefore setting a cutoff at around .2 or a bit more seems optimal. However, this is a hindsight bias and too risky to do if we dont have more data to test it on. Ideally we would an intial training set to compare models, another to tweak parameters and decision threholds, and a test set to test them all.&lt;/p&gt;
&lt;p&gt;Let’s end by simply the test set performances of all models together.&lt;/p&gt;
&lt;center&gt;
&lt;img src = &#34;/performance.png&#34; width = 600 height = 600&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The best models are the C50 and random forest models built on the expanded set of predictors. Since we are interested sensitivity (true positives for churn = “yes”), we can improve the performance of our predictions by altering the decision threshold cutoffs. But, since we do not have a separate training set of data to test the new cutoff on, doing so would create model bias. All in all, predicting churn was an informative exercise!&lt;/p&gt;
&lt;p&gt;Note: If you wish to see code, feel free to check the code for this post on my github.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Classification Models - Hepatic Dataset</title>
      <link>/blog/2018-12-03-linear-classification-models---hepatic-dataset/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-12-03-linear-classification-models---hepatic-dataset/</guid>
      <description>&lt;p&gt;This post is following exercise 1 in Chapter 12 of &lt;a href=&#34;http://appliedpredictivemodeling.com&#34;&gt;Applied Predicative Modeling&lt;/a&gt;. Here I use the machine learning package CARET in R to make classification models; in particular, the linear classification models discussed in Chapter 12.&lt;/p&gt;
&lt;p&gt;The dataset in question is about hepatic injury (liver damage). It includes a dataframe of biological related predictors of liver damage &lt;code&gt;bio&lt;/code&gt;, a dataframe of chemical related predictors &lt;code&gt;chem&lt;/code&gt;, and the response variable we are interested in, &lt;code&gt;injury&lt;/code&gt;. If a model can be fitted adequately, that model could potentially be used to screen for harmful compounds in the future.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)

library(pROC)

library(AppliedPredictiveModeling)

data(hepatic)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before fitting a model it’s always necessary to preprocess data. For linear classification algorithms, this means&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Remove near zero variance predictor variables/ near-zero variance) predictors.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Remove collinear variables&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##remove problematic variables for &amp;#39;bio&amp;#39; dataframe

bio_zerovar &amp;lt;- nearZeroVar(bio)

bio_collinear &amp;lt;- findLinearCombos(cov(bio))$remove

bio &amp;lt;- bio[, -c(bio_zerovar, bio_collinear)]

##remove problematic variables for &amp;#39;chem&amp;#39; dataframe

chem_zerovar &amp;lt;- nearZeroVar(chem)

chem_collinear &amp;lt;- findLinearCombos(chem)$remove

chem &amp;lt;- chem[, -c(chem_zerovar, chem_collinear)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;injury&lt;/code&gt; response variable I am fitting to has three classes - “None”, “Mild”, and “Severe”. If the response variable has too many classes, it can (somewhat subjectively) be trimmed down. For expediency’s sake, I decide to collapse &lt;code&gt;injury&lt;/code&gt; into 2 classes - “Yes” or “No”, where I count mild injuries as “No”. (&lt;strong&gt;Warning&lt;/strong&gt; This might influence prediction negatively, so in the future I’ll be sure to try multi-class probability predictions.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Collapse response into &amp;quot;Yes&amp;quot; or &amp;quot;No&amp;quot; values

lut &amp;lt;- c(&amp;quot;None&amp;quot; = &amp;quot;No&amp;quot;, &amp;quot;Mild&amp;quot; = &amp;quot;No&amp;quot;, &amp;quot;Severe&amp;quot; = &amp;quot;Yes&amp;quot;)

injury2 &amp;lt;- factor(unname(lut[injury]))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, I should consider a few other questions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How to partition data with unbalanced classes (“No” far outnumbers “Yes” values in &lt;code&gt;injury&lt;/code&gt;.)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How to validate model results&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Which metric to maximize for the best model (Accuracy, Sensitivity, Specificity)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thankfully, makes data partitioning easy with &lt;code&gt;createDataPartition&lt;/code&gt;, which automatically uses stratified sampling to help control for severe class imbalances. As for validation, I choose for each model to be cross validated using 10 fold repeat cross validation, specified in the &lt;code&gt;trainControl&lt;/code&gt; command. (Although my first model will not need cross validation, the others will, so I’ll simply use the same control for each model.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)

##Partition the data with stratified sampling

training &amp;lt;- createDataPartition(injury2, p = .8, list = FALSE)

##Partition train and test sets for bio data

bio_train &amp;lt;- bio[training, ]

bio_test &amp;lt;- bio[-training, ]

##Partition for train and test sets for chemical data

chem_train &amp;lt;- chem[training, ]

chem_test &amp;lt;- chem[-training, ]

##Partition for the train and test sets for the response variable

injury_train &amp;lt;- injury2[training]

injury_test &amp;lt;- injury2[-training]

## Set training control for model building

ctrl &amp;lt;- trainControl(method = &amp;quot;repeatedcv&amp;quot;, 10, repeats = 10, summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s important to decide what goal to train my models for - Accuracy, Sensitivity, or Specificity? For this, you also need to know what your “positive” variable is from Caret’s perspective. Caret chooses the first factor class as its “positive” value, which corresponds to “No” in my &lt;code&gt;injury&lt;/code&gt; vector. Therefore, Sensitivity corresponds to amount of “No” values correctly predicted, whereas Specificity equates to the “Yes” values correctly predicted.&lt;/p&gt;
&lt;p&gt;Deciding between those choices, it seems most important to build a model that can predict the “Yes” values - cases result in hepatic damage. This makes sense, a mistake in a model predicting no liver damage would tragic, so we should do everything we can to capture the “Yes” values as much as possible, even if it means sacrificing accuracy. A look at the data:&lt;/p&gt;
&lt;p&gt;The first model is a classifier using linear discriminant analysis. I apply a model for the biological indicators as well as the chemical indicators, to see which have better predicative power.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_lda &amp;lt;- train(bio_train, injury_train, method = &amp;quot;lda&amp;quot;, trControl = ctrl, metric = &amp;quot;Spec&amp;quot;)

chem_lda &amp;lt;- train(chem_train, injury_train, method = &amp;quot;lda&amp;quot;, trControl = ctrl, metric = &amp;quot;Spec&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Generate LDA model predictions for bio indicator test set

bio_lda_pred &amp;lt;- predict(bio_lda, bio_test)

##Generate confusion matrix to show results

confusionMatrix(bio_lda_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  44   3
##        Yes  6   3
##                                           
##                Accuracy : 0.8393          
##                  95% CI : (0.7167, 0.9238)
##     No Information Rate : 0.8929          
##     P-Value [Acc &amp;gt; NIR] : 0.9278          
##                                           
##                   Kappa : 0.3115          
##  Mcnemar&amp;#39;s Test P-Value : 0.5050          
##                                           
##             Sensitivity : 0.8800          
##             Specificity : 0.5000          
##          Pos Pred Value : 0.9362          
##          Neg Pred Value : 0.3333          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7857          
##    Detection Prevalence : 0.8393          
##       Balanced Accuracy : 0.6900          
##                                           
##        &amp;#39;Positive&amp;#39; Class : No              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the “non-information rate” is .89, meaning that if we randomly guessed “No” each time, the model would automatically be right 89% of the time. Accuracy here is .83, which appears to underperform. But remember, accuracy isn’t important - predicting true “Yes” values correctly is.&lt;/p&gt;
&lt;p&gt;For the biological predictors, we get .5 for Specificity, correctly identifying 3 Yes values but also generating 3 false negative predictions for 3 other Yes values – I hope other models can do better.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Chem predictor LDA model

chem_lda_pred &amp;lt;- predict(chem_lda, chem_test)

##confusion matrix

confusionMatrix(chem_lda_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  41   5
##        Yes  9   1
##                                           
##                Accuracy : 0.75            
##                  95% CI : (0.6163, 0.8561)
##     No Information Rate : 0.8929          
##     P-Value [Acc &amp;gt; NIR] : 0.9994          
##                                           
##                   Kappa : -0.0103         
##  Mcnemar&amp;#39;s Test P-Value : 0.4227          
##                                           
##             Sensitivity : 0.8200          
##             Specificity : 0.1667          
##          Pos Pred Value : 0.8913          
##          Neg Pred Value : 0.1000          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7321          
##    Detection Prevalence : 0.8214          
##       Balanced Accuracy : 0.4933          
##                                           
##        &amp;#39;Positive&amp;#39; Class : No              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LDA for chemical predictors fares worse at predicting the true Yes values. Here Specificity only reaches .16.&lt;/p&gt;
&lt;p&gt;Now lets try some other models for comparison. The first will be a penalized logistic regression model. For alpha values of 1, and lambda 0, it will behave like a lasso model, whereas with alpha 0 and a non-zero lambda, a ridge regression model. Anywhere in between is an elastic net. Here, I don’t specify a tuning grid, I just let Caret come up with a list of parameters for me, with `tuneLength = 20’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_plr &amp;lt;- train(bio_train, injury_train, method = &amp;quot;glmnet&amp;quot;, family = &amp;quot;binomial&amp;quot;, metric = &amp;quot;Spec&amp;quot;, tuneLength = 20,

                 trControl = ctrl)

chem_plr &amp;lt;- train(chem_train, injury_train, method = &amp;quot;glmnet&amp;quot;, family = &amp;quot;binomial&amp;quot;, metric = &amp;quot;Spec&amp;quot;, tuneLength = 20, trControl = ctrl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_plr_pred &amp;lt;- predict(bio_plr, bio_test)

confusionMatrix(bio_plr_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  43   4
##        Yes  7   2
##                                           
##                Accuracy : 0.8036          
##                  95% CI : (0.6757, 0.8977)
##     No Information Rate : 0.8929          
##     P-Value [Acc &amp;gt; NIR] : 0.9858          
##                                           
##                   Kappa : 0.1585          
##  Mcnemar&amp;#39;s Test P-Value : 0.5465          
##                                           
##             Sensitivity : 0.8600          
##             Specificity : 0.3333          
##          Pos Pred Value : 0.9149          
##          Neg Pred Value : 0.2222          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7679          
##    Detection Prevalence : 0.8393          
##       Balanced Accuracy : 0.5967          
##                                           
##        &amp;#39;Positive&amp;#39; Class : No              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This penalized logistic regression doesn’t perform as well as simple linear discriminant analysis for the bio predictors.&lt;/p&gt;
&lt;p&gt;And now the chem predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chem_plr_pred &amp;lt;- predict(chem_plr, chem_test)

confusionMatrix(chem_plr_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  44   6
##        Yes  6   0
##                                           
##                Accuracy : 0.7857          
##                  95% CI : (0.6556, 0.8841)
##     No Information Rate : 0.8929          
##     P-Value [Acc &amp;gt; NIR] : 0.9945          
##                                           
##                   Kappa : -0.12           
##  Mcnemar&amp;#39;s Test P-Value : 1.0000          
##                                           
##             Sensitivity : 0.8800          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8800          
##          Neg Pred Value : 0.0000          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7857          
##    Detection Prevalence : 0.8929          
##       Balanced Accuracy : 0.4400          
##                                           
##        &amp;#39;Positive&amp;#39; Class : No              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Penalized logistic regression fares even worse for chemical predictors. Clearly this is not a strong model to capture the structure of the data for the pattern we’re looking for.&lt;/p&gt;
&lt;p&gt;Now, to fit a partial least squares regression model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_pls &amp;lt;- train(bio_train, injury_train, method = &amp;quot;pls&amp;quot;, trControl = ctrl, metric = &amp;quot;Spec&amp;quot;, tuneLength = 20)

chem_pls &amp;lt;- train(chem_train, injury_train, method = &amp;quot;pls&amp;quot;, trControl = ctrl, metric = &amp;quot;Spec&amp;quot;, tuneLength = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_pls_pred &amp;lt;- predict(bio_pls, bio_test)

confusionMatrix(bio_pls_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  45   5
##        Yes  5   1
##                                          
##                Accuracy : 0.8214         
##                  95% CI : (0.696, 0.9109)
##     No Information Rate : 0.8929         
##     P-Value [Acc &amp;gt; NIR] : 0.9664         
##                                          
##                   Kappa : 0.0667         
##  Mcnemar&amp;#39;s Test P-Value : 1.0000         
##                                          
##             Sensitivity : 0.9000         
##             Specificity : 0.1667         
##          Pos Pred Value : 0.9000         
##          Neg Pred Value : 0.1667         
##              Prevalence : 0.8929         
##          Detection Rate : 0.8036         
##    Detection Prevalence : 0.8929         
##       Balanced Accuracy : 0.5333         
##                                          
##        &amp;#39;Positive&amp;#39; Class : No             
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Only .16 for Specificity achieved here.&lt;/p&gt;
&lt;p&gt;And for the chemical predictors, we get 0 as seen below. Chemical indicators are continuously faring worse than biological predictors at predicting hepatic injury, &lt;strong&gt;and there is still no great model&lt;/strong&gt;, so far.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chem_pls_pred &amp;lt;- predict(chem_pls, chem_test, probability = TRUE)

confusionMatrix(chem_pls_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  47   6
##        Yes  3   0
##                                           
##                Accuracy : 0.8393          
##                  95% CI : (0.7167, 0.9238)
##     No Information Rate : 0.8929          
##     P-Value [Acc &amp;gt; NIR] : 0.9278          
##                                           
##                   Kappa : -0.0769         
##  Mcnemar&amp;#39;s Test P-Value : 0.5050          
##                                           
##             Sensitivity : 0.9400          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8868          
##          Neg Pred Value : 0.0000          
##              Prevalence : 0.8929          
##          Detection Rate : 0.8393          
##    Detection Prevalence : 0.9464          
##       Balanced Accuracy : 0.4700          
##                                           
##        &amp;#39;Positive&amp;#39; Class : No              
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_centroid &amp;lt;- train(bio_train, injury_train, method = &amp;quot;pam&amp;quot;,

                      trControl = ctrl, preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;), metric = &amp;quot;Spec&amp;quot;, tuneLength = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chem_centroid &amp;lt;- train(chem_train, injury_train, method = &amp;quot;pam&amp;quot;,

                       trControl = ctrl, preProcess = c(&amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;), metric = &amp;quot;Spec&amp;quot;, tuneLength = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_centroid_pred &amp;lt;- predict(bio_centroid, bio_test)

chem_centroid_pred &amp;lt;- predict(chem_centroid, chem_test)

confusionMatrix(bio_centroid_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  50   6
##        Yes  0   0
##                                           
##                Accuracy : 0.8929          
##                  95% CI : (0.7812, 0.9597)
##     No Information Rate : 0.8929          
##     P-Value [Acc &amp;gt; NIR] : 0.60647         
##                                           
##                   Kappa : 0               
##  Mcnemar&amp;#39;s Test P-Value : 0.04123         
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8929          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.8929          
##          Detection Rate : 0.8929          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &amp;#39;Positive&amp;#39; Class : No              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;0 for specificity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confusionMatrix(chem_centroid_pred, injury_test, positive = &amp;quot;No&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  45   5
##        Yes  5   1
##                                          
##                Accuracy : 0.8214         
##                  95% CI : (0.696, 0.9109)
##     No Information Rate : 0.8929         
##     P-Value [Acc &amp;gt; NIR] : 0.9664         
##                                          
##                   Kappa : 0.0667         
##  Mcnemar&amp;#39;s Test P-Value : 1.0000         
##                                          
##             Sensitivity : 0.9000         
##             Specificity : 0.1667         
##          Pos Pred Value : 0.9000         
##          Neg Pred Value : 0.1667         
##              Prevalence : 0.8929         
##          Detection Rate : 0.8036         
##    Detection Prevalence : 0.8929         
##       Balanced Accuracy : 0.5333         
##                                          
##        &amp;#39;Positive&amp;#39; Class : No             
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And .16. for Specificity here, yawn.&lt;/p&gt;
&lt;p&gt;So the &lt;strong&gt;best model for predicting hepatic injury&lt;/strong&gt; turns out to be the &lt;strong&gt;first fit&lt;/strong&gt;, the LDA model on the biological indicators.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predictions_bio_lda &amp;lt;- predict(bio_lda, bio_test, type = &amp;quot;prob&amp;quot;)

pROC::plot.roc(injury_test, predictions_bio_lda$Yes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-03-linear-classification-models-hepatic-dataset_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;However, the area under the curve is not as high as I’d wish. Perhaps in the future I’ll revisit this data and see what can be done differently to predict injury.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>