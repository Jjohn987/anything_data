<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width,minimum-scale=1,maximum-scale=1">

  
    
    
      <link href="/css/fonts.css" rel="stylesheet" type="text/css">
    
  

  
  <title>Linear Classification Models - Hepatic Dataset</title>

  
  
  <link rel="stylesheet" href="/css/hugo-octopress.css">

  
  

  
    <link rel="stylesheet" href="/css/fork-awesome.min.css">
  

  
  <link href="/favicon.png" rel="icon">

  
  
  

  

  <meta name="description" content="My R blog, with projects in analytics, text analysis, data mining, and visualization. I might occasionally include Chinese text.">
  <meta name="keywords" content="">

  <meta name="author" content="Jeremy Johnson">

  
  <meta name="generator" content="Hugo 0.40.2" />

  
  
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-118999723-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


  
  

</head>
<body>


<header role="banner"><hgroup>
  
  <h1><a href="/">Anything is |Data| is Anything</a></h1>
    <h2>The investigation of things...</h2>
</hgroup></header>


<nav role="navigation">
<fieldset class="mobile-nav">
  
  <select onchange="location = this.value;">
    <option value="">Navigate…</option>
      

  </select>
</fieldset>


<ul class="main-navigation">
  
  
</ul>


<ul class="subscription">
  

</ul>


</nav>


<div id="main">
  <div id="content">
    <div>
      <article class="hentry" role="article">

        
        

<header>
    <p class="meta">Dec 3, 2018
         - 24 minute read 
         - <a href="/blog/2018-12-03-linear-classification-models---hepatic-dataset/#disqus_thread">Comments</a>

        
        
        
            - <a class="label" href="/categories/modelling/">modelling </a><a class="label" href="/categories/machine-learning/">machine learning </a>
        
    </p>
    <h1 class="entry-title">
         Linear Classification Models - Hepatic Dataset 
    </h1>
</header>


        <div class="entry-content">
          
          
          
          
          <p>This post is following exercise 1 in Chapter 12 of <a href="http://appliedpredictivemodeling.com">Applied Predicative Modeling</a>. Here I use the machine learning package CARET in R to make classification models; in particular, the linear classification models discussed in Chapter 12.</p>
<p>The dataset in question is about hepatic injury (liver damage). It includes a dataframe of biological related predictors of liver damage <code>bio</code>, a dataframe of chemical related predictors <code>chem</code>, and the response variable we are interested in, <code>injury</code>. If a model can be fitted adequately, that model could potentially be used to screen for harmful compounds in the future.</p>
<pre class="r"><code>library(caret)

library(pROC)

library(AppliedPredictiveModeling)

data(hepatic)</code></pre>
<p>Before fitting a model it’s always necessary to preprocess data. For linear classification algorithms, this means</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>Remove near zero variance predictor variables/ near-zero variance) predictors.</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Remove collinear variables</li>
</ol></li>
</ul>
<pre class="r"><code>##remove problematic variables for &#39;bio&#39; dataframe

bio_zerovar &lt;- nearZeroVar(bio)

bio_collinear &lt;- findLinearCombos(cov(bio))$remove

bio &lt;- bio[, -c(bio_zerovar, bio_collinear)]

##remove problematic variables for &#39;chem&#39; dataframe

chem_zerovar &lt;- nearZeroVar(chem)

chem_collinear &lt;- findLinearCombos(chem)$remove

chem &lt;- chem[, -c(chem_zerovar, chem_collinear)]</code></pre>
<p>The <code>injury</code> response variable I am fitting to has three classes - “None”, “Mild”, and “Severe”. If the response variable has too many classes, it can (somewhat subjectively) be trimmed down. For expediency’s sake, I decide to collapse <code>injury</code> into 2 classes - “Yes” or “No”, where I count mild injuries as “No”. (<strong>Warning</strong> This might influence prediction negatively, so in the future I’ll be sure to try multi-class probability predictions.)</p>
<pre class="r"><code>##Collapse response into &quot;Yes&quot; or &quot;No&quot; values

lut &lt;- c(&quot;None&quot; = &quot;No&quot;, &quot;Mild&quot; = &quot;No&quot;, &quot;Severe&quot; = &quot;Yes&quot;)

injury2 &lt;- factor(unname(lut[injury]))</code></pre>
<p>Now, I should consider a few other questions.</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>How to partition data with unbalanced classes (“No” far outnumbers “Yes” values in <code>injury</code>.)</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>How to validate model results</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>Which metric to maximize for the best model (Accuracy, Sensitivity, Specificity)</li>
</ol></li>
</ul>
<p>Thankfully, makes data partitioning easy with <code>createDataPartition</code>, which automatically uses stratified sampling to help control for severe class imbalances. As for validation, I choose for each model to be cross validated using 10 fold repeat cross validation, specified in the <code>trainControl</code> command. (Although my first model will not need cross validation, the others will, so I’ll simply use the same control for each model.)</p>
<pre class="r"><code>set.seed(1234)

##Partition the data with stratified sampling

training &lt;- createDataPartition(injury2, p = .8, list = FALSE)

##Partition train and test sets for bio data

bio_train &lt;- bio[training, ]

bio_test &lt;- bio[-training, ]

##Partition for train and test sets for chemical data

chem_train &lt;- chem[training, ]

chem_test &lt;- chem[-training, ]

##Partition for the train and test sets for the response variable

injury_train &lt;- injury2[training]

injury_test &lt;- injury2[-training]

## Set training control for model building

ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, 10, repeats = 10, summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE)</code></pre>
<p>It’s important to decide what goal to train my models for - Accuracy, Sensitivity, or Specificity? For this, you also need to know what your “positive” variable is from Caret’s perspective. Caret chooses the first factor class as its “positive” value, which corresponds to “No” in my <code>injury</code> vector. Therefore, Sensitivity corresponds to amount of “No” values correctly predicted, whereas Specificity equates to the “Yes” values correctly predicted.</p>
<p>Deciding between those choices, it seems most important to build a model that can predict the “Yes” values - cases result in hepatic damage. This makes sense, a mistake in a model predicting no liver damage would tragic, so we should do everything we can to capture the “Yes” values as much as possible, even if it means sacrificing accuracy. A look at the data:</p>
<pre class="r"><code>head(str(bio_train))[1:5]</code></pre>
<pre><code>## &#39;data.frame&#39;:    225 obs. of  102 variables:
##  $ Z1  : num  4 4 4.9 4 4 4 4 4 4 5.4 ...
##  $ Z4  : num  6 6 4 6 6 5.2 6 6 6 6 ...
##  $ Z6  : num  4 4 4.9 4.5 4 4.5 4 4 4 4 ...
##  $ Z7  : num  4 4 4.5 4.5 4 4 4.5 4 4 4 ...
##  $ Z10 : num  4.5 4.5 4 4 4.5 5.3 4 4 4 4.5 ...
##  $ Z11 : num  4 4 4 4.5 4 4.5 4 4 4 4 ...
##  $ Z12 : num  4 4 4 4 4.5 4.5 4 4 4 4 ...
##  $ Z15 : num  4 4 6.9 4.5 4 4.5 5.3 4 4 4 ...
##  $ Z17 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z18 : num  4 4 4.9 4.5 4 4.5 4 4 4 4 ...
##  $ Z19 : num  4.5 4 5.5 6.5 4 6.5 4 4 4 4 ...
##  $ Z20 : num  4.5 4 4 4 4.5 4 4 4 4 4.5 ...
##  $ Z21 : num  4 4 4 4.5 4 4 4 4 4 4 ...
##  $ Z23 : num  4 4 4 4 4 4.5 4 4 4.5 4 ...
##  $ Z26 : num  4 4 5.2 4 4 5.4 4 5.9 4 4 ...
##  $ Z28 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z33 : int  0 4 4 4 4 0 4 4 4 4 ...
##  $ Z34 : num  4 4 6 5.9 4 6.3 4 4 4 4 ...
##  $ Z35 : num  4 4 6.3 4 4 5.7 4 4 4 4 ...
##  $ Z36 : num  4 4 4 5.2 4 7.5 4 4 4 4 ...
##  $ Z38 : num  4 4 5.8 4 4 4.5 4 4 4 4 ...
##  $ Z40 : num  4 4 5.5 6.1 5.3 5.5 4.5 4 4 4.5 ...
##  $ Z42 : num  4 4.5 4 5.6 4 4.5 4 4 4 4 ...
##  $ Z43 : num  4 4 4.5 4 4 4 4 4 4 4 ...
##  $ Z44 : num  4 4 6 4.5 4 4 4 4 4.5 4.5 ...
##  $ Z46 : num  4 4 5.1 4.5 4 5.7 4 4.3 4 4 ...
##  $ Z48 : num  4 4 4.9 4.5 4 5.7 4 4 4 4 ...
##  $ Z50 : num  4 4 4.5 4 4 4.9 4 4 4 4 ...
##  $ Z51 : num  4 4 5 5.4 4 7.2 4 4 4 4 ...
##  $ Z52 : num  4 4 4.7 4.9 4 6.2 4 4 4 4 ...
##  $ Z53 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z56 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z57 : num  4 4 4 4 4 4 4 4 4 4.5 ...
##  $ Z58 : num  4 4 4 4 4 5.4 4 4 4 4 ...
##  $ Z59 : num  4 4 4 4 4 4 4.5 4 4 4 ...
##  $ Z61 : num  4 4 4 4 4 0 4 4 4 4 ...
##  $ Z64 : num  4 4 5.3 5.1 4 6.8 4 4 4 4 ...
##  $ Z68 : num  4 4 4.6 5.6 4 5.9 4.5 4 4 4 ...
##  $ Z69 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z71 : num  4 4 5.5 5 4 6 4 4 4 4 ...
##  $ Z75 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z79 : num  4 4 4 4 4 4.2 4 4 4 4 ...
##  $ Z80 : num  4 4 0 4 4 4 4 4 4 4 ...
##  $ Z82 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z83 : num  4 4 4 4 4.5 4.5 4 4 4 4.5 ...
##  $ Z85 : num  4 4 4.5 4 4 4 4 4 4 4 ...
##  $ Z93 : num  4 4 4 4 4 4 4 4.5 4 4 ...
##  $ Z94 : num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z97 : num  4.4 4 5.1 7.2 4 5.4 5.1 4 4 4 ...
##  $ Z98 : num  4 4 6.1 6.2 4 4 4.5 4 4 4 ...
##  $ Z99 : num  4 4 5.5 4 4 4 4 4 4 4 ...
##  $ Z100: num  4.9 4 4.5 4 4 4 4 4 4.5 4 ...
##  $ Z101: num  4 4 5.5 4 4 4 4 4 4 4 ...
##  $ Z102: num  4 4 4.5 4 4 4 4 4 4 4 ...
##  $ Z103: num  4 4 5.1 7.1 4 5.2 4.5 4 4 4 ...
##  $ Z105: num  4 4 4.5 4 4 4 4 4 4 4 ...
##  $ Z111: num  4 4 4.5 4 4 4.5 4 5.1 4 4 ...
##  $ Z113: num  4 4 4 4 4 4 4 4 4 4.5 ...
##  $ Z116: num  4 4.5 5.1 4 4 4.5 4 4 4.5 4 ...
##  $ Z117: num  4 4 5.9 7 4 6.7 4 4 4 4 ...
##  $ Z118: num  4.5 4 5.4 7.5 4 6 5.2 4 4 4 ...
##  $ Z121: num  4 4 4.9 4 4 4 4 4 4 4 ...
##  $ Z122: num  4 4 4 4 4 4 4 4.8 4 4 ...
##  $ Z125: num  4.5 4 4.5 4 4 4.5 4 4 4 4 ...
##  $ Z126: num  4 4 6 4.5 4 5.4 4 4 4 4 ...
##  $ Z127: num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z128: num  4 4 4 4 4 5.1 4 4 4 4.5 ...
##  $ Z130: num  4 4.5 4.5 7.1 4 7.4 4 4.5 4 4 ...
##  $ Z131: num  4 4 5.1 4 4 4 4 5 4 4 ...
##  $ Z132: num  4 4 4 4 4 6.9 4 4 4 4 ...
##  $ Z133: num  4 4 4 4 4 4.5 4 4 4 4 ...
##  $ Z134: num  6.8 4 5.2 5.3 4 5.4 5.1 4 4 4 ...
##  $ Z135: num  4 4 5.4 4 4 4 4 4 4 4 ...
##  $ Z136: num  4 4 4 4 4 4 4.5 4 4 4 ...
##  $ Z137: num  4 4 6.2 5.5 4 6.8 5 4 4 4 ...
##  $ Z138: num  4 4 5.3 4 4 6 4 4 4 4 ...
##  $ Z140: num  4 4 4 4 4 4.3 4 4 4 4 ...
##  $ Z142: num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z144: num  4 4 4 4 4 4 4 5.1 4 4 ...
##  $ Z145: num  7.4 4 4 5.1 4 5.6 5 4 4 4 ...
##  $ Z146: num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z147: num  4 4 4 4 4 4.8 4 4 4 4 ...
##  $ Z148: num  4.5 4 5.6 6 4 4.9 4.6 4 4 4 ...
##  $ Z149: num  4 4 5.6 7.4 4 6.7 4 4.5 4 4 ...
##  $ Z151: num  4 4 4.9 5.1 4 6.9 4 4 4 4 ...
##  $ Z152: num  4 4 4 5.1 4 8 4.5 4 4 4 ...
##  $ Z153: num  4 4 4 4 4 6 4 4 4 4 ...
##  $ Z155: num  4 4 4.8 5.3 4 7.1 4 4 4 4 ...
##  $ Z156: num  4 4 4 4.9 4 4 4 4 4.5 4 ...
##  $ Z157: num  4 4 4 4.5 4 5.1 4 4 4 4 ...
##  $ Z158: num  4 4 5.2 6.7 4 6.7 4 4 4 4 ...
##  $ Z159: num  4 4 0 4 4 4 4 4 4 4 ...
##  $ Z163: num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z164: num  4 4 4 4.8 4 4 4 4 4 4 ...
##  $ Z167: num  4 4 4 4 4 4.4 4 4 4 4 ...
##  $ Z169: num  4 4 4 4.5 4 4 4.5 4 4 4 ...
##  $ Z173: num  4 4 4 4 4 4 4 4 4 4 ...
##  $ Z174: num  5 4 4.6 6.9 4 6.8 4 4 4 4 ...
##  $ Z176: num  4 4 4 4 4.5 4 4.5 4 4 4.5 ...
##   [list output truncated]</code></pre>
<pre><code>## NULL</code></pre>
<pre class="r"><code>head(str(chem_train))[1:5]</code></pre>
<pre><code>## &#39;data.frame&#39;:    225 obs. of  114 variables:
##  $ X1  : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ X2  : num  0.153 -2.046 3.657 2.615 -0.114 ...
##  $ X3  : int  0 0 0 0 0 0 0 1 0 0 ...
##  $ X4  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X5  : int  15 12 25 22 14 34 15 10 28 19 ...
##  $ X6  : int  4 6 4 4 4 6 4 3 4 9 ...
##  $ X7  : int  11 6 21 18 10 28 11 7 24 10 ...
##  $ X11 : int  4 6 5 4 4 6 4 3 4 9 ...
##  $ X13 : int  9 12 11 10 8 13 9 10 28 10 ...
##  $ X14 : int  4 6 4 4 4 4 4 3 4 5 ...
##  $ X19 : int  6 0 14 12 6 18 6 0 0 2 ...
##  $ X20 : int  3 5 3 4 2 7 2 2 27 4 ...
##  $ X21 : int  10 5 18 17 9 27 9 2 27 6 ...
##  $ X22 : int  6 0 14 12 4 22 5 1 21 4 ...
##  $ X23 : int  3 0 6 9 3 12 3 0 0 2 ...
##  $ X25 : int  0 0 0 0 0 0 2 3 0 0 ...
##  $ X26 : int  2 0 2 2 0 0 1 1 3 0 ...
##  $ X27 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X28 : int  3 6 5 4 4 3 2 2 2 5 ...
##  $ X29 : int  0 0 0 0 0 4 0 0 16 0 ...
##  $ X30 : int  1 0 1 1 1 2 1 0 0 1 ...
##  $ X31 : int  0 0 0 0 0 0 0 0 2 0 ...
##  $ X33 : int  0 0 4 0 0 2 0 0 0 2 ...
##  $ X34 : int  1 0 1 1 1 2 1 0 0 0 ...
##  $ X35 : int  0 0 1 0 0 0 0 0 0 1 ...
##  $ X36 : int  2 0 0 2 2 0 2 0 0 0 ...
##  $ X37 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X38 : int  0 0 1 0 0 1 0 0 0 0 ...
##  $ X39 : int  0 0 0 0 0 0 0 1 1 0 ...
##  $ X40 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X47 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X49 : int  1 0 2 1 1 1 1 1 0 1 ...
##  $ X50 : int  0 0 0 0 0 2 0 0 0 4 ...
##  $ X51 : int  0 0 0 0 0 1 0 0 0 0 ...
##  $ X52 : int  0 0 0 0 0 0 0 0 0 3 ...
##  $ X61 : int  1 0 0 1 1 0 1 0 0 0 ...
##  $ X62 : int  0 0 1 0 0 0 0 0 0 1 ...
##  $ X63 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X64 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X65 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X66 : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ X68 : int  0 0 1 0 0 1 0 1 0 0 ...
##  $ X69 : int  0 0 0 0 0 0 0 0 0 1 ...
##  $ X72 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X81 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X82 : int  3 6 2 3 3 1 3 2 4 4 ...
##  $ X83 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X84 : int  3 6 1 2 2 0 1 0 2 3 ...
##  $ X85 : int  2 0 0 1 2 0 0 0 0 0 ...
##  $ X87 : int  0 0 0 0 0 0 0 0 1 0 ...
##  $ X88 : int  0 0 0 1 1 0 2 1 0 1 ...
##  $ X89 : int  0 0 0 0 1 0 0 0 0 1 ...
##  $ X90 : int  0 0 0 1 0 0 2 0 0 0 ...
##  $ X91 : int  0 0 0 0 0 0 0 1 0 0 ...
##  $ X97 : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X99 : int  0 0 0 0 0 0 0 1 1 0 ...
##  $ X101: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X102: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X103: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X104: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X105: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X106: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X109: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X113: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X118: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X120: int  4 5 7 7 1 7 4 4 4 2 ...
##  $ X121: int  15 11 27 23 15 38 15 9 31 21 ...
##  $ X124: int  0 0 0 0 6 6 0 0 20 5 ...
##  $ X126: int  4 6 4 4 4 2 4 2 4 8 ...
##  $ X127: int  6 15 5 6 6 2 5 0 6 22 ...
##  $ X130: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X131: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X132: int  0 0 0 0 0 1 0 0 1 1 ...
##  $ X133: int  0 0 0 0 0 0 0 1 1 2 ...
##  $ X134: int  2 5 0 1 1 0 1 0 0 1 ...
##  $ X135: int  2 5 1 2 3 0 1 0 0 5 ...
##  $ X136: int  2 5 1 2 3 0 1 0 0 7 ...
##  $ X137: int  0 4 0 0 0 0 0 0 0 1 ...
##  $ X138: int  0 4 1 0 0 0 1 0 0 5 ...
##  $ X139: int  0 4 1 0 0 0 1 0 0 9 ...
##  $ X140: int  1 3 0 0 0 0 0 0 0 1 ...
##  $ X141: int  1 3 0 0 1 1 2 0 0 3 ...
##  $ X142: int  1 3 0 0 1 0 4 0 0 6 ...
##  $ X143: int  3 2 2 2 1 0 0 0 1 2 ...
##  $ X144: int  3 2 3 3 3 2 1 0 1 9 ...
##  $ X145: int  3 2 3 3 3 2 1 0 1 13 ...
##  $ X146: int  1 0 0 0 1 1 1 0 0 0 ...
##  $ X147: int  1 0 1 0 1 0 1 0 0 0 ...
##  $ X148: int  2 0 3 2 2 4 2 0 0 0 ...
##  $ X149: int  0 0 0 0 0 0 0 0 1 3 ...
##  $ X150: int  0 0 0 0 0 0 0 0 0 3 ...
##  $ X151: int  0 0 0 0 0 2 0 0 5 3 ...
##  $ X153: int  0 0 0 0 0 1 0 0 0 0 ...
##  $ X154: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X155: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X157: int  0 0 0 0 2 2 0 0 0 2 ...
##  $ X159: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ X160: int  0 0 0 0 0 0 0 0 5 5 ...
##  $ X161: int  0 0 0 0 6 6 0 0 12 0 ...
##   [list output truncated]</code></pre>
<pre><code>## NULL</code></pre>
<p>The first model is a classifier using linear discriminant analysis. I apply a model for the biological indicators as well as the chemical indicators, to see which have better predicative power.</p>
<pre class="r"><code>bio_lda &lt;- train(bio_train, injury_train, method = &quot;lda&quot;, trControl = ctrl, metric = &quot;Spec&quot;)

chem_lda &lt;- train(chem_train, injury_train, method = &quot;lda&quot;, trControl = ctrl, metric = &quot;Spec&quot;)</code></pre>
<pre class="r"><code>##Generate LDA model predictions for bio indicator test set

bio_lda_pred &lt;- predict(bio_lda, bio_test)

##Generate confusion matrix to show results

confusionMatrix(bio_lda_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  44   3
##        Yes  6   3
##                                           
##                Accuracy : 0.8393          
##                  95% CI : (0.7167, 0.9238)
##     No Information Rate : 0.8929          
##     P-Value [Acc &gt; NIR] : 0.9278          
##                                           
##                   Kappa : 0.3115          
##  Mcnemar&#39;s Test P-Value : 0.5050          
##                                           
##             Sensitivity : 0.8800          
##             Specificity : 0.5000          
##          Pos Pred Value : 0.9362          
##          Neg Pred Value : 0.3333          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7857          
##    Detection Prevalence : 0.8393          
##       Balanced Accuracy : 0.6900          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>Note that the “non-information rate” is .89, meaning that if we randomly guessed “No” each time, the model would automatically be right 89% of the time. Accuracy here is .83, which appears to underperform. But remember, accuracy isn’t important - predicting true “Yes” values correctly is.</p>
<p>For the biological predictors, we get .5 for Specificity, correctly identifying 3 Yes values but also generating 3 false negative predictions for 3 other Yes values – I hope other models can do better.</p>
<pre class="r"><code>##Chem predictor LDA model

chem_lda_pred &lt;- predict(chem_lda, chem_test)

##confusion matrix

confusionMatrix(chem_lda_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  41   5
##        Yes  9   1
##                                           
##                Accuracy : 0.75            
##                  95% CI : (0.6163, 0.8561)
##     No Information Rate : 0.8929          
##     P-Value [Acc &gt; NIR] : 0.9994          
##                                           
##                   Kappa : -0.0103         
##  Mcnemar&#39;s Test P-Value : 0.4227          
##                                           
##             Sensitivity : 0.8200          
##             Specificity : 0.1667          
##          Pos Pred Value : 0.8913          
##          Neg Pred Value : 0.1000          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7321          
##    Detection Prevalence : 0.8214          
##       Balanced Accuracy : 0.4933          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>LDA for chemical predictors fares worse at predicting the true Yes values. Here Specificity only reaches .16.</p>
<p>Now lets try some other models for comparison. The first will be a penalized logistic regression model. For alpha values of 1, and lambda 0, it will behave like a lasso model, whereas with alpha 0 and a non-zero lambda, a ridge regression model. Anywhere in between is an elastic net. Here, I don’t specify a tuning grid, I just let Caret come up with a list of parameters for me, with `tuneLength = 20’.</p>
<pre class="r"><code>bio_plr &lt;- train(bio_train, injury_train, method = &quot;glmnet&quot;, family = &quot;binomial&quot;, metric = &quot;Spec&quot;, tuneLength = 20,

                 trControl = ctrl)

chem_plr &lt;- train(chem_train, injury_train, method = &quot;glmnet&quot;, family = &quot;binomial&quot;, metric = &quot;Spec&quot;, tuneLength = 20, trControl = ctrl)</code></pre>
<pre class="r"><code>bio_plr_pred &lt;- predict(bio_plr, bio_test)

confusionMatrix(bio_plr_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  43   4
##        Yes  7   2
##                                           
##                Accuracy : 0.8036          
##                  95% CI : (0.6757, 0.8977)
##     No Information Rate : 0.8929          
##     P-Value [Acc &gt; NIR] : 0.9858          
##                                           
##                   Kappa : 0.1585          
##  Mcnemar&#39;s Test P-Value : 0.5465          
##                                           
##             Sensitivity : 0.8600          
##             Specificity : 0.3333          
##          Pos Pred Value : 0.9149          
##          Neg Pred Value : 0.2222          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7679          
##    Detection Prevalence : 0.8393          
##       Balanced Accuracy : 0.5967          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>This penalized logistic regression doesn’t perform as well as simple linear discriminant analysis for the bio predictors.</p>
<p>And now the chem predictors.</p>
<pre class="r"><code>chem_plr_pred &lt;- predict(chem_plr, chem_test)

confusionMatrix(chem_plr_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  44   6
##        Yes  6   0
##                                           
##                Accuracy : 0.7857          
##                  95% CI : (0.6556, 0.8841)
##     No Information Rate : 0.8929          
##     P-Value [Acc &gt; NIR] : 0.9945          
##                                           
##                   Kappa : -0.12           
##  Mcnemar&#39;s Test P-Value : 1.0000          
##                                           
##             Sensitivity : 0.8800          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8800          
##          Neg Pred Value : 0.0000          
##              Prevalence : 0.8929          
##          Detection Rate : 0.7857          
##    Detection Prevalence : 0.8929          
##       Balanced Accuracy : 0.4400          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>Penalized logistic regression fares even worse for chemical predictors. Clearly this is not a strong model to capture the structure of the data for the pattern we’re looking for.</p>
<p>Now, to fit a partial least squares regression model.</p>
<pre class="r"><code>bio_pls &lt;- train(bio_train, injury_train, method = &quot;pls&quot;, trControl = ctrl, metric = &quot;Spec&quot;, tuneLength = 20)

chem_pls &lt;- train(chem_train, injury_train, method = &quot;pls&quot;, trControl = ctrl, metric = &quot;Spec&quot;, tuneLength = 20)</code></pre>
<pre class="r"><code>bio_pls_pred &lt;- predict(bio_pls, bio_test)

confusionMatrix(bio_pls_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  45   5
##        Yes  5   1
##                                          
##                Accuracy : 0.8214         
##                  95% CI : (0.696, 0.9109)
##     No Information Rate : 0.8929         
##     P-Value [Acc &gt; NIR] : 0.9664         
##                                          
##                   Kappa : 0.0667         
##  Mcnemar&#39;s Test P-Value : 1.0000         
##                                          
##             Sensitivity : 0.9000         
##             Specificity : 0.1667         
##          Pos Pred Value : 0.9000         
##          Neg Pred Value : 0.1667         
##              Prevalence : 0.8929         
##          Detection Rate : 0.8036         
##    Detection Prevalence : 0.8929         
##       Balanced Accuracy : 0.5333         
##                                          
##        &#39;Positive&#39; Class : No             
## </code></pre>
<p>Only .16 for Specificity achieved here.</p>
<p>And for the chemical predictors, we get 0 as seen below. Chemical indicators are continuously faring worse than biological predictors at predicting hepatic injury, <strong>and there is still no great model</strong>, so far.</p>
<pre class="r"><code>chem_pls_pred &lt;- predict(chem_pls, chem_test, probability = TRUE)

confusionMatrix(chem_pls_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  47   6
##        Yes  3   0
##                                           
##                Accuracy : 0.8393          
##                  95% CI : (0.7167, 0.9238)
##     No Information Rate : 0.8929          
##     P-Value [Acc &gt; NIR] : 0.9278          
##                                           
##                   Kappa : -0.0769         
##  Mcnemar&#39;s Test P-Value : 0.5050          
##                                           
##             Sensitivity : 0.9400          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8868          
##          Neg Pred Value : 0.0000          
##              Prevalence : 0.8929          
##          Detection Rate : 0.8393          
##    Detection Prevalence : 0.9464          
##       Balanced Accuracy : 0.4700          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<pre class="r"><code>bio_centroid &lt;- train(bio_train, injury_train, method = &quot;pam&quot;,

                      trControl = ctrl, preProcess = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;Spec&quot;, tuneLength = 20)</code></pre>
<pre><code>## 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111</code></pre>
<pre class="r"><code>chem_centroid &lt;- train(chem_train, injury_train, method = &quot;pam&quot;,

                       trControl = ctrl, preProcess = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;Spec&quot;, tuneLength = 20)</code></pre>
<pre><code>## 12345678910111213141516171819202122232425262728293011111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111</code></pre>
<pre class="r"><code>bio_centroid_pred &lt;- predict(bio_centroid, bio_test)

chem_centroid_pred &lt;- predict(chem_centroid, chem_test)

confusionMatrix(bio_centroid_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  50   6
##        Yes  0   0
##                                           
##                Accuracy : 0.8929          
##                  95% CI : (0.7812, 0.9597)
##     No Information Rate : 0.8929          
##     P-Value [Acc &gt; NIR] : 0.60647         
##                                           
##                   Kappa : 0               
##  Mcnemar&#39;s Test P-Value : 0.04123         
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.0000          
##          Pos Pred Value : 0.8929          
##          Neg Pred Value :    NaN          
##              Prevalence : 0.8929          
##          Detection Rate : 0.8929          
##    Detection Prevalence : 1.0000          
##       Balanced Accuracy : 0.5000          
##                                           
##        &#39;Positive&#39; Class : No              
## </code></pre>
<p>0 for specificity.</p>
<pre class="r"><code>confusionMatrix(chem_centroid_pred, injury_test, positive = &quot;No&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction No Yes
##        No  45   5
##        Yes  5   1
##                                          
##                Accuracy : 0.8214         
##                  95% CI : (0.696, 0.9109)
##     No Information Rate : 0.8929         
##     P-Value [Acc &gt; NIR] : 0.9664         
##                                          
##                   Kappa : 0.0667         
##  Mcnemar&#39;s Test P-Value : 1.0000         
##                                          
##             Sensitivity : 0.9000         
##             Specificity : 0.1667         
##          Pos Pred Value : 0.9000         
##          Neg Pred Value : 0.1667         
##              Prevalence : 0.8929         
##          Detection Rate : 0.8036         
##    Detection Prevalence : 0.8929         
##       Balanced Accuracy : 0.5333         
##                                          
##        &#39;Positive&#39; Class : No             
## </code></pre>
<p>And .16. for Specificity here, yawn.</p>
<p>So the <strong>best model for predicting hepatic injury</strong> turns out to be the <strong>first fit</strong>, the LDA model on the biological indicators.</p>
<pre class="r"><code>predictions_bio_lda &lt;- predict(bio_lda, bio_test, type = &quot;prob&quot;)

pROC::plot.roc(injury_test, predictions_bio_lda$Yes)</code></pre>
<p><img src="/post/2018-12-03-linear-classification-models-hepatic-dataset_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>However, the area under the curve is not as high as I’d wish. Perhaps in the future I’ll revisit this data and see what can be done differently to predict injury.</p>

        </div>
        

<footer>
  <p class="meta">
    <span class="byline author vcard">Posted by <span class="fn">Jeremy Johnson</span></span>
    
    <time>Dec 3, 2018</time>
    
      <span class="categories">
        Tags:
        
          <a class="category" href="/tags/classification">classification</a>  <a class="category" href="/tags/linear-model">linear model</a>  <a class="category" href="/tags/caret">caret</a>  <a class="category" href="/tags/lda">LDA</a>  <a class="category" href="/tags/pls">PLS</a>  <a class="category" href="/tags/nearest-shrunken-centroids">nearest shrunken centroids</a>  <a class="category" href="/tags/logistic-regression">logistic regression</a>  
    
    </span>
  </p>

  
  

  

  <p class="meta">
    
        <a class="basic-alignment left" href="/blog/2018-09-21-first-app---halo-5-stats/" title="First App - Halo 5 Stats">First App - Halo 5 Stats</a>
    

    
  </p>
  
    
      <div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
    
    
    
    if (window.location.hostname == "localhost")
        return;

    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    var disqus_shortname = 'anything-is-data-is-anything';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    
  
</footer>

      </article>
    </div>
    

<aside class="sidebar thirds">
  <section class="first odd">

    
      <h1>About Me</h1>
    

    <p>
      
        <p>I&rsquo;m Jeremy, and am deeply interested in data science. Especially R.
  <br><br>
  But my educational background is in <strong>Chinese</strong> and German. <strong>Think that&rsquo;s unrelated with R</strong>? <br>
  <br> Well, I see language is a door to another world, and data science as a skillset to understanding it. <br><br> <strong>Anything is Data, and Data is Anything</strong>.
  </br><br>
  <em>Cool Websites</em> <br>
  <a href="https://www.r-bloggers.com">https://www.r-bloggers.com</a> <br></p>

<p><strong>Me on Other Platforms:</strong>  </br></p>

      
    </p>
  </section>



  
  <ul class="sidebar-nav">
    <li class="sidebar-nav-item">
      <a target="_blank" href="https://github.com/Jjohn987/" title="https://github.com/Jjohn987/"><i class="fa fa-github fa-3x"></i></a>
      
      
      
      
         
      
      <a target="_blank" href="https://www.linkedin.com/in/jeremy-johnson-09016a112" title="https://www.linkedin.com/in/jeremy-johnson-09016a112"><i class="fa fa-linkedin fa-3x"></i></a>
      <a target="_blank" href="https://stackexchange.com/users/11094434/superball10000" title="https://stackexchange.com/users/11094434/superball10000"><i class="fa fa-stack-overflow fa-3x"></i></a>
      
      
      
      
      

    
    
    </li>
  </ul>

  

  

  
  
  
    
      <section class="even">
        <h1>Recent Posts</h1>
        <ul id="recent_posts">
          
          
            
              <li class="post">
                <a href="/blog/2018-12-03-linear-classification-models---hepatic-dataset/">Linear Classification Models - Hepatic Dataset</a>
              </li>
            
          
            
              <li class="post">
                <a href="/blog/2018-09-21-first-app---halo-5-stats/">First App - Halo 5 Stats</a>
              </li>
            
          
            
              <li class="post">
                <a href="/blog/2018-06-08-part-ii-chinese-classics-word/network-plots/">Part II: Chinese Classics&#39; Word/Network Plots</a>
              </li>
            
          
            
              <li class="post">
                <a href="/blog/2018-06-01-plotting-word-bigrams-with-3-chinese-classics/">Plotting Word Bigrams with 3 Chinese Classics</a>
              </li>
            
          
            
              <li class="post">
                <a href="/blog/2018-05-29-a-tidytext-analysis-of-3-chinese-classics/">A Tidytext Analysis of 3 Chinese Classics</a>
              </li>
            
          
        </ul>
      </section>
    
  
</aside>

  </div>
</div>

<footer role="contentinfo">
  <p>Copyright &copy; 2018 Jeremy Johnson - <a href="/license/">License</a> -
  <span class="credit">Powered by <a target="_blank" href="https://gohugo.io">Hugo</a> and <a target="_blank" href="https://github.com/parsiya/hugo-octopress/">Hugo-Octopress</a> theme.
</p>

</footer>






</body>
</html>

