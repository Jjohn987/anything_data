<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scraping on Anything is |Data| is Anything</title>
    <link>/tags/scraping/</link>
    <description>Recent content in Scraping on Anything is |Data| is Anything</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 May 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/scraping/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Scraping for a Booklist of the Chinese Classics</title>
      <link>/blog/2018-05-08-scraping-for-a-booklist-of-the-chinese-classics/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-08-scraping-for-a-booklist-of-the-chinese-classics/</guid>
      <description>&lt;p&gt;Last week I was considering a project that would be interesting and unique. I decided I would like to do a text analysis on classical Chinese texts, but wasn’t sure what kind of analysis regarding which texts. I decided to keep it small - and use five of the “core” Chinese classics - The Analects, The Mengzi, Dao De Jing, Zhuangzi, and Mozi. While there are many books in Confucianism, Daoism, and Moism, these texts are often used as the most representative examples of each “genre”.&lt;/p&gt;
&lt;p&gt;Of course, the first key question was, &lt;strong&gt;from where can I get the data?&lt;/strong&gt; One website with a rich amount of Chinese text data regarding the classics is &lt;a href=&#34;https://ctext.org/&#34;&gt;ctext.org&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/ctextscreen.png&#34; alt=&#34;A screenshot of Ctext.org&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;A screenshot of Ctext.org&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;But when looking at the site design, I wondered “How can I get this in R?” Scraping wasn’t entirely feasible due to the terms outlawing this practice. Secondly, scraping is a bit of a delicate operation - if text isn’t formated uniformly across pages then you might be in for a headache. You also don’t want to give the server unnecessary stress. In the end, if you opt for it, you’ll have to make your functions work with the site structure - and as evidenced by the screenshot, it seemed a bit… messy. (Well, it turns out that actually it wasn’t.)&lt;/p&gt;
&lt;p&gt;To get the text of the Chinese classics into R, the solution was to build an API. There is an API avaialble on ctext.org’s website, but it’s made in Python. I’ve never built an API or proto-API functions before, but the latter was easier than I thought. Right now I’ll save that for a future post.&lt;/p&gt;
&lt;p&gt;To wrap up this post - Many of the key functions in the site API revolve around passing a book or chapter as the args. So, it turned out scraping was a necessary evil. Therefore I kept it limited and not too demanding.&lt;/p&gt;
&lt;p&gt;Without ado, here is the (very limited) scraping I did to create a book list with chapters, which I put to use later in my homemade API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rvest)
library(stringr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;first-scrape&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First Scrape&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 1st Scrape - Get list of books available on ctext website. 
url &amp;lt;- &amp;quot;https://ctext.org/&amp;quot;

path &amp;lt;- read_html(url)
genre_data &amp;lt;- path %&amp;gt;%
  html_nodes(css = &amp;quot;.container &amp;gt; .etext&amp;quot;) %&amp;gt;%
  html_attr(&amp;quot;href&amp;quot;)

##Delete first observation which is not a genre
genre_data &amp;lt;- genre_data[-1] %&amp;gt;% tibble(&amp;quot;genre&amp;quot; = .)
##Append the base url to the sub-links
genre_data &amp;lt;- genre_data %&amp;gt;%
  mutate(genre_links = paste(&amp;quot;https://ctext.org&amp;quot;, &amp;quot;/&amp;quot;, genre_data[[1]], sep = &amp;quot;&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;bindrcpp&amp;#39; was built under R version 3.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I set up a scraping function which needs to iterate over each book from the “genre_data” dataframe just created. Note the “Sys.sleep” call at the end to avoid overloading the server and play nicely with the website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;function---preparing-for-the-2nd-scrape&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Function - Preparing for the 2nd scrape&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##2nd Scrape - Make function to apply to each book, to get chapters
scraping_function &amp;lt;- function(genre, genre_links) {
  url &amp;lt;- genre_links[[1]]
  path &amp;lt;- read_html(url)
  
  data &amp;lt;- path %&amp;gt;%
    html_nodes(css = &amp;quot;#content3 &amp;gt; a&amp;quot;) %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;)
  
  genre &amp;lt;- genre
  data &amp;lt;- data_frame(data, genre)
  
  ##Some string cleaning with stringr and mutate commands
  data &amp;lt;- data %&amp;gt;% mutate(book = str_extract(data, &amp;quot;^[a-z].*[\\/]&amp;quot;)) %&amp;gt;%
    mutate(book = str_replace(book, &amp;quot;\\/&amp;quot;, &amp;quot;&amp;quot;))
  data &amp;lt;- data %&amp;gt;%
    mutate(chapter = str_extract(data, &amp;quot;[\\/].*$&amp;quot;)) %&amp;gt;%
    mutate(chapter = str_replace(chapter, &amp;quot;/&amp;quot;, &amp;quot;&amp;quot;))
  data &amp;lt;- data %&amp;gt;%
    mutate(links = paste(&amp;quot;https://ctext.org/&amp;quot;, book, &amp;quot;/&amp;quot;, chapter, sep = &amp;quot;&amp;quot;))
  data &amp;lt;- data %&amp;gt;% select(-data) %&amp;gt;%
    filter(complete.cases(.))

  Sys.sleep(2.5)
  data
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If there was one takeaway from writing that function, it was that I should deepen my proficiency in regex. Finding the right regular expressions to capture the book and chapter names wasn’t HARD, but I did have to make several attempts before getting it all right. Previously web content was clean enough that I didn’t have to do this. Anyway, let’s apply the hard work to our original genre dataframe so that we can get a dataframe of books and their chapters. It’s going to be a big one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-the-function-and-get-the-data..-i-have-come-to-love-purrr-for-this.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply the function and get the data.. I have come to love &lt;code&gt;purrr&lt;/code&gt; for this.&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Apply function to genre_data dataframe, create a data frame of books and chapters

all_works &amp;lt;- map2(genre_data$genre, genre_data$genre_links, ~ scraping_function(..1, ..2))

book_list &amp;lt;- all_works %&amp;gt;% do.call(rbind, .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here it is. The final variable “book_list” is a collection of books and chapters of each book, as listed on Ctext.org.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(book_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##          genre     book       chapter
##          &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;
## 1 confucianism analects        xue-er
## 2 confucianism analects     wei-zheng
## 3 confucianism analects         ba-yi
## 4 confucianism analects        li-ren
## 5 confucianism analects gong-ye-chang
## 6 confucianism analects       yong-ye
## # ... with 1 more variables: links &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is clearly in long format (convenient but not necessary, in fact this more a side effect of my scraping)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(book_list)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;tbl_df&amp;#39;, &amp;#39;tbl&amp;#39; and &amp;#39;data.frame&amp;#39;:    5869 obs. of  4 variables:
##  $ genre  : chr  &amp;quot;confucianism&amp;quot; &amp;quot;confucianism&amp;quot; &amp;quot;confucianism&amp;quot; &amp;quot;confucianism&amp;quot; ...
##  $ book   : chr  &amp;quot;analects&amp;quot; &amp;quot;analects&amp;quot; &amp;quot;analects&amp;quot; &amp;quot;analects&amp;quot; ...
##  $ chapter: chr  &amp;quot;xue-er&amp;quot; &amp;quot;wei-zheng&amp;quot; &amp;quot;ba-yi&amp;quot; &amp;quot;li-ren&amp;quot; ...
##  $ links  : chr  &amp;quot;https://ctext.org/analects/xue-er&amp;quot; &amp;quot;https://ctext.org/analects/wei-zheng&amp;quot; &amp;quot;https://ctext.org/analects/ba-yi&amp;quot; &amp;quot;https://ctext.org/analects/li-ren&amp;quot; ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is quite lengthy at nearly 6,000 rows and 130 different books. And this is an important dataframe which I will use in my API that I make, to pull textual data into R from Ctext.org.&lt;/p&gt;
&lt;p&gt;Next post, I plan on sharing the process and results of my Chinese Classics text analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Top MBA Programs by US News</title>
      <link>/blog/2017-07-01-top-mba-programs-by-us-news/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-07-01-top-mba-programs-by-us-news/</guid>
      <description>&lt;p&gt;Somebody once asked me for reccomendations on MBA programs based on rank and tuition. I didn’t have any information on hand, but knew how toget it. Webscraping.&lt;/p&gt;
&lt;p&gt;Webscraping is an immensly useful tool for gathering data from webpages, when it isn’t hosted on an API or stored in a file somewhere. R’s best tool for webscraping is &lt;strong&gt;Rvest.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So I decided to scrape information on the US News website for university rankings, which has at least 20 pages of MBA probrams available. To copy and paste that much data into a spreadsheet would be annoying and quite an eye strain.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(stringr)
library(rvest)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, before writing a scraper, one needs to code it according to the page layout.&lt;/p&gt;
&lt;p&gt;I find that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are 19 pages of information I need.&lt;/li&gt;
&lt;li&gt;Everything is directly available on those pages, no need to iterate over additional, internal links.&lt;/li&gt;
&lt;li&gt;Xpath selectors perform better than CSS selectors in this particular example.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will use lapply to run through all 19 pages, with the sprintf function to help paste the new page number in each time, for each new iteration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 19 pages of MBA programs on US News website.
pages &amp;lt;- 1:19

get_usnews_mbas &amp;lt;- function(x) {
  website1 &amp;lt;- &amp;#39;https://www.usnews.com/best-graduate-schools/top-business-schools/mba-rankings/page+%s&amp;#39;
  url &amp;lt;- sprintf(website1, x, collapse = &amp;quot;&amp;quot;)
  website &amp;lt;- read_html(url)
  base_url &amp;lt;- &amp;#39;http://www.usnews.com&amp;#39;
  
  University &amp;lt;- website %&amp;gt;%
    html_nodes(&amp;#39;.school-name&amp;#39;) %&amp;gt;%
    html_text()
 
   Location &amp;lt;- website %&amp;gt;%
    html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;article&amp;quot;]/table/tbody/tr/td[2]/p&amp;#39;) %&amp;gt;%
    html_text()
 
    Link &amp;lt;- website %&amp;gt;%
    html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;article&amp;quot;]/table/tbody/tr/td[2]/a&amp;#39;) %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    str_trim(side = &amp;quot;both&amp;quot;)
 
     Tuition &amp;lt;- website %&amp;gt;%
    html_nodes(xpath = &amp;#39;//*[@id=&amp;quot;article&amp;quot;]/table/tbody/tr/td[3]&amp;#39;) %&amp;gt;%
    str_replace_all(&amp;quot;\n&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
    str_replace_all(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;) %&amp;gt;%
    str_extract(&amp;quot;\\d+&amp;quot;) %&amp;gt;%
    as.integer()
  
  ##Combine vectors into data frame
  data_frame(University,
             Location,
             Tuition,
             Link)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;apply-the-function-get-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply the function, get the data!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;USNEWS_MBAS &amp;lt;- do.call(rbind, lapply(pages, get_usnews_mbas))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rvest made this feel almost like magic - Pulling it into R without having to do any manual clicking, copying, and pasting. As I said, web scraping is very powerful!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean the Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Split location into City and State.

USNEWS_MBAS &amp;lt;- USNEWS_MBAS %&amp;gt;%
  separate(Location, c(&amp;quot;City&amp;quot;, &amp;quot;State&amp;quot;), sep = &amp;quot;,&amp;quot;)

##Create column for rankings... 

USNEWS_MBAS &amp;lt;- USNEWS_MBAS %&amp;gt;%
  mutate(Rank = 1: n())

##The URL&amp;#39;s didnt scrape 100% correctly. But it is easy to paste the base URL onto each branch.
base_url &amp;lt;- &amp;#39;www.usnews.com&amp;#39;

USNEWS_MBAS &amp;lt;- USNEWS_MBAS %&amp;gt;%
  mutate(base_url = base_url) %&amp;gt;%
  unite(Links, base_url, Link, sep = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s enough data cleaning, but adding a variable to segment or classify the schools into brackets of ten could be useful when visualizing them in terms of rank and tuition cost later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;USNEWS_MBAS &amp;lt;- USNEWS_MBAS %&amp;gt;%
  mutate(Tier = cut(Rank, breaks = seq(0, 400, by = 10))) %&amp;gt;%
  mutate(Tier = str_replace(Tier, &amp;quot;,&amp;quot;, &amp;quot;-&amp;quot;)) %&amp;gt;% 
  mutate(Tier = str_replace_all(Tier, &amp;quot;[^0-9-]&amp;quot;, &amp;quot;&amp;quot;))

##Convert intervals into factors  

USNEWS_MBAS$Tier &amp;lt;- factor(USNEWS_MBAS$Tier, levels = c(&amp;quot;0-10&amp;quot;, &amp;quot;10-20&amp;quot;, &amp;quot;20-30&amp;quot;, &amp;quot;30-40&amp;quot;, &amp;quot;40-50&amp;quot;, &amp;quot;50-60&amp;quot;, &amp;quot;60-70&amp;quot;, &amp;quot;70-80&amp;quot;, &amp;quot;80-90&amp;quot;, &amp;quot;90-100&amp;quot;, &amp;quot;Out of Top 100&amp;quot;))

USNEWS_MBAS %&amp;gt;%
  select(University, City, State, Tuition, Rank, Tier, Links)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 475 x 7
##    University        City   State Tuition  Rank Tier  Links               
##    &amp;lt;chr&amp;gt;             &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;               
##  1 Harvard Universi… Boston &amp;quot; MA&amp;quot;   72000     1 0-10  www.usnews.com/best…
##  2 University of Ch… Chica… &amp;quot; IL&amp;quot;   69200     2 0-10  www.usnews.com/best…
##  3 University of Pe… Phila… &amp;quot; PA&amp;quot;   70200     3 0-10  www.usnews.com/best…
##  4 Stanford Univers… Stanf… &amp;quot; CA&amp;quot;   68868     4 0-10  www.usnews.com/best…
##  5 Massachusetts In… Cambr… &amp;quot; MA&amp;quot;   71000     5 0-10  www.usnews.com/best…
##  6 Northwestern Uni… Evans… &amp;quot; IL&amp;quot;   68955     6 0-10  www.usnews.com/best…
##  7 University of Ca… Berke… &amp;quot; CA&amp;quot;   58794     7 0-10  www.usnews.com/best…
##  8 University of Mi… Ann A… &amp;quot; MI&amp;quot;   62300     8 0-10  www.usnews.com/best…
##  9 Columbia Univers… New Y… &amp;quot; NY&amp;quot;   71544     9 0-10  www.usnews.com/best…
## 10 Dartmouth Colleg… Hanov… &amp;quot; NH&amp;quot;   68910    10 0-10  www.usnews.com/best…
## # ... with 465 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s filter the schools and grab only the top 100.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;USNEWS_MBAS %&amp;gt;%
  filter(Rank &amp;lt;= 100) %&amp;gt;%
  ggplot(aes(x = Rank, y = Tuition, color = Tier)) + geom_point() +
  ggtitle(&amp;quot;American MBA Programs&amp;quot;, subtitle = &amp;quot;By Rank and Tuition&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-06-top-mba-programs-by-us-news_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Some top 20-30 schools look to be a good deal in terms of high rank and (relatively) lower tuition. But if one goes for schools ranked in the 30-40 range, then the tuition gets even lower.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-more-detailed-look&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A more detailed look&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;USNEWS_MBAS %&amp;gt;%
  select(University, Rank, Tuition, Tier) %&amp;gt;%
  arrange(Rank, Tuition) %&amp;gt;%
  group_by(Tier) %&amp;gt;%
  top_n(-3, Tuition) %&amp;gt;%
  ggplot(aes(x = reorder(University, -Rank), y = Tuition, fill = Tier)) +
  geom_bar(stat = &amp;quot;identity&amp;quot;) + 
coord_flip() +
  ggtitle(&amp;quot;Three &amp;#39;Cheapest&amp;#39; Schools per Tier&amp;quot;, subtitle = &amp;quot;MBA Programs&amp;quot;) +
  xlab(&amp;quot;University&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-05-06-top-mba-programs-by-us-news_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Up above, I selected 3 institutions from each “Tier” of rankings with the lowest tuition and plotted them. Some universities have suspiciously low tuition, which is likely due to documentation error on the US News website.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-observations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Some Observations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MBA programs are very expensive for any institution ranked from 1-30.&lt;/li&gt;
&lt;li&gt;Programs become affordable from ranks 30-50 and onward&lt;/li&gt;
&lt;li&gt;Anything that appears especially low is probably an inconsistency in US News’ tuition data.&lt;/li&gt;
&lt;li&gt;It’d be better to compare school rankings across a certain program instead of comprehensively&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Migrated from my original Wordpress blog&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>